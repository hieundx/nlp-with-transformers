{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy search encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec976a3e56a4a248970aac8f0c31cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbbbb0cc30541c8ad677bdac38c0e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c638362adb4639a53e120ef94f8ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f696aa2cec6471ab830076bfc7334d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fd4cdc47e64e2a89a5cbf2b399a3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcac4bdc06f94328bd30a26344e5d0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67ab7ab1b234d44ae8bd9fba9f3c180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = 'Transformers are the'\n",
    "input_ids = tokenizer(input_txt, return_tensors='pt').input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers are the most popular toy line in the world, and the Transformers'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(input_ids, max_length):\n",
    "    ids = input_ids\n",
    "    with torch.no_grad():\n",
    "        for _ in range (max_length):\n",
    "            output = model(ids)\n",
    "            next_token_logits = output.logits[0, -1, :]\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.argmax(next_token_probs, dim=-1)\n",
    "            ids = torch.cat([ids, next_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "    text = tokenizer.decode(ids[0])\n",
    "    return text\n",
    "generate_text(input_ids, max_length=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.7388,  4.8211,  1.7156,  ..., -6.9956, -4.9382, -0.3056],\n",
      "         [ 4.6024,  7.0256,  2.0684,  ..., -8.2232, -4.8386,  2.6589],\n",
      "         [ 1.9228,  0.9972, -3.3385,  ..., -3.4645, -3.6465, -0.4307],\n",
      "         [ 0.3186,  0.8100, -2.9974,  ..., -3.2712, -4.3837,  0.8114]]])\n",
      "torch.Size([1, 4, 50257])\n",
      "tensor([[ 2.7388,  4.8211,  1.7156,  ..., -6.9956, -4.9382, -0.3056],\n",
      "        [ 4.6024,  7.0256,  2.0684,  ..., -8.2232, -4.8386,  2.6589],\n",
      "        [ 1.9228,  0.9972, -3.3385,  ..., -3.4645, -3.6465, -0.4307],\n",
      "        [ 0.3186,  0.8100, -2.9974,  ..., -3.2712, -4.3837,  0.8114]])\n",
      "torch.Size([4, 50257])\n",
      "torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits)\n",
    "print(output.logits.shape)\n",
    "print(output.logits[0])\n",
    "print(output.logits[0].shape)\n",
    "print(output.logits[0, -1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5103e-05, 2.4688e-05, 5.4820e-07,  ..., 4.1690e-07, 1.3704e-07,\n",
       "        2.4722e-05])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits = output.logits[0, -1, :]\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "next_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(749)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' most'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_char = tokenizer.decode([next_token])\n",
    "next_token_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the `generate` method to auto decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world, and the Transformers\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors='pt').input_ids.to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy encodig is better for generating deterministic text, such as math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"1+1=\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=2, do_sample=False)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors='pt').input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "logp = F.log_softmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.1888e-02,  1.1745e+00, -2.8043e+00,  ..., -6.1697e+00,\n",
      "          -7.5409e+00, -8.9564e-01],\n",
      "         [-9.2064e-01,  1.5377e-01, -3.6908e+00,  ..., -6.0280e+00,\n",
      "          -5.7153e+00, -1.6073e-02],\n",
      "         [ 1.0610e+00,  1.7739e+00, -2.7187e+00,  ..., -5.4062e+00,\n",
      "          -8.3658e+00, -6.4379e-01],\n",
      "         ...,\n",
      "         [ 7.0360e-01,  3.5427e+00, -1.1218e+00,  ..., -6.2427e+00,\n",
      "          -5.3343e+00,  7.8475e+00],\n",
      "         [-9.0571e+00, -4.7897e+00, -8.3393e+00,  ..., -1.4334e+01,\n",
      "          -1.6138e+01, -5.3552e+00],\n",
      "         [ 2.9631e+00,  1.2243e+01,  6.6012e+00,  ..., -4.3975e+00,\n",
      "          -3.0764e+00,  1.8207e+00]]])\n",
      "torch.Size([1, 47, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits)\n",
    "print(logp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probs_from_logits(logits_up_to_last, tokens_from_second):\n",
    "    logp = F.log_softmax(logits_up_to_last, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, tokens_from_second.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprob(model, tokens, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)\n",
    "        logits_up_to_last = output.logits[:, :-1, :]\n",
    "        tokens_from_second = tokens[:, 1:]\n",
    "        \n",
    "        log_probs = log_probs_from_logits(logits_up_to_last,tokens_from_second)\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log prob for greedy search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n",
      "Log prob = -87.43272\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print('Log prob =', logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log prob for beam search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "\n",
      "The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n",
      "Log prob = -55.225967\n"
     ]
    }
   ],
   "source": [
    "output_beam= model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print('Log prob =', logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use n-gram penalty to reduce repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n",
      "Log prob = -93.12165\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print('Log prob =', logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      " irreversible beastxy, weapons tempm 105m compression streamuploadoil − 346 soon billed glaciersayers sometimezo Though infall seasons tast combined pow preventunc Missing Draco haven probability confidential midsburn�owners buttons Identification 2016worldawongriorsunic Rufust jobs Mills Alexa Commodore is extQuestion Fifaه�― Costume switching201ranch campus GET bananaselaison reason feminine Personal animppodcastinion Fully immersive podcast International\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High temp results in too much gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers from the University of New Mexico, who are part of the National Science Foundation, have been studying the area for the past 30 years. They had already discovered the existence of a herd of unicorns in the area, but the discovery of the language-learning unicorns, was a surprise.\n",
      "\n",
      "\n",
      "\"We were not expecting to find them,\" said Dr. María-Elena Ramí\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low temp results in more coherrent text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k and top-p sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "After a two year study, researchers from the U.K.'s Oxford University concluded; 'The evidence was clear that the animal was still in the virgin forests of Chacaltaya, Peru at the end of the last Ice Age when they were last visited by archaeologists, probably around 4000 to 4000 BC.'\n",
      "\n",
      "'The evidence was clear that the animal was still in the virgin forests of Chac\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,top_k=50)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "While some say that the unicorns' unusual intelligence, ability to speak English and their ability to understand human speech is nothing new, but that the scientists who made this discovery are the first to find evidence of it since it was first described in 1869.\n",
      "\n",
      "\n",
      "The researchers, Dr. Enrique D. Borsa, Dr. Sergio Ibarra, Dr. Ana Ponce de León and\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
    " top_p=0.90)\n",
    "print(tokenizer.decode(output_topp[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
